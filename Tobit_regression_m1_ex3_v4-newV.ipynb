{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobit regression with TensorFlow\n",
    "\n",
    "Tobit regression fits the following model for non-negative data $ y $: \n",
    "\n",
    "$  y({\\bf X}) = \\max (0, w_0 + \\sum_{i=1}^{N} w_i X_i + w_{N+1} \\cdot \\varepsilon) $  \n",
    "\n",
    "Here $ X_i $ are predictors, $ \\varepsilon \\sim N(0,1) $ is a standard Gaussian noise, and $ w_{N+1} $ is the noise\n",
    "volatility (standard deviation).\n",
    "\n",
    "Our problem is to fit parameters $ N+2 $ parameters $ w_{i} $ for $ i = 0, \\ldots, N+1 $ to the observed set of pairs $ \\left({\\bf X}_i, y_i \\right) $  \n",
    "\n",
    "We use synthetic data with known parameters to learn how to implement Tobit Regression in TensorFlow. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "- The token generated by Coursera (COURSERA_TOKEN) expires every <b>30 minutes</b>. It is advisable to always work with the most recent generated token so as to avoid any submission related errors. If you receive such error messages, rerun the cells containing your code and the GRADED FUNCTION in the same order. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(≈ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
    "\n",
    "- The blue button \"Submit Assignment\" does not work. After running all the cells, please go directly to Assignment-> My submission to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"w3Hc-vZdEeehlBIKDnZryg\" \n",
    "all_parts=[\"pLnY5\", \"RKR6p\", \"IU1pw\", \"ISVtY\", \"Cutr3\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "COURSERA_EMAIL = # the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function  to reset the TF graph to the same state each time\n",
    "def reset_graph(seed=42):\n",
    "    # to make results reproducible across runs\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobit Regression class\n",
    "\n",
    "**Instructions**:\n",
    "Complete the code for the calculation of loss function (the negative log-likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tobit_Regression:\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.005, L=0):\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, [None, n_features], name=\"Input\")\n",
    "        self.target = tf.placeholder(tf.float32, [None, 1], name=\"Target\")\n",
    "        \n",
    "        # the first weight is for the intercept, the last one is for a square root of the noise std \n",
    "        self.weights = tf.Variable(tf.random_normal([n_features + 2, 1]))\n",
    "        \n",
    "        # Augmented data matrix is obtained by adding a column of ones to the data matrix\n",
    "        self.data_plus_bias = tf.concat([tf.ones([tf.shape(self.input)[0], 1]), self.input], axis=1)\n",
    "\n",
    "        #######################################################################\n",
    "        # MLE for Tobit regression \n",
    "        \n",
    "        # noise volatility is obtained as a square of the last weight to ensure positivity \n",
    "        self.sigma = 0.0001 + tf.square(self.weights[-1])\n",
    "        \n",
    "        # term1 and term2 are just placeholders initialized such that the code runs\n",
    "        # students need to initialize them appropriately to solve this assignment\n",
    "        term1 = tf.Variable(np.zeros(shape=(n_features + 2, 1)))\n",
    "        term2 = tf.Variable(np.zeros(shape=(n_features + 2, 1)))\n",
    "        # THIS IS THE PART THAT STUDENTS ARE SUPPOSED TO WRITE THEMSELVES TO COMPLETE THE IMPLEMENTATION \n",
    "        # OF THE TOBIT REGRESSION MODEL\n",
    "        \n",
    "        # FOR THE ASSIGNMENT: complete the code for the calculation of loss function \n",
    "        # (the negative log-likelihood)\n",
    "        ### START CODE HERE ### (≈ 6-7 lines of code)\n",
    "\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        self.loss = - tf.reduce_mean(term1 + term2)\n",
    "        \n",
    "        #####################################################################\n",
    "\n",
    "        # Use Adam optimization for training\n",
    "        self.train_step = (tf.train.AdamOptimizer(learning_rate).minimize(self.loss), -self.loss)\n",
    "        \n",
    "        # prediction made from the model: Use a ReLU neuron!\n",
    "        self.output = tf.nn.relu(tf.matmul(self.data_plus_bias[:, :], self.weights[:-1]))\n",
    "        \n",
    "        # Check the output L1-norm error  \n",
    "        self.output_L1_error = tf.reduce_mean(tf.abs(self.target - self.output))\n",
    "\n",
    "    def generate_data(n_points,\n",
    "                      n_features,\n",
    "                      weights,\n",
    "                      noise_std):\n",
    "\n",
    "        # Bounds of [-1,1] in space of n_points x n_features\n",
    "        np.random.seed(42)\n",
    "        bias = np.ones(n_points).reshape((-1,1))\n",
    "        low = - np.ones((n_points,n_features),'float')\n",
    "        high = np.ones((n_points,n_features),'float')\n",
    "\n",
    "        # simulated features are uniformally distributed on [-1,1].\n",
    "        # The size n_points x n_features of array X is inferred by broadcasting of 'low' and 'high'\n",
    "        X = np.random.uniform(low=low, high=high)\n",
    "        \n",
    "        # simulated noise\n",
    "        noise = np.random.normal(size=(n_points, 1))\n",
    "        \n",
    "        # outputs    \n",
    "        Y = weights[0] * bias + np.dot(X, weights[1:]).reshape((-1,1)) + noise_std * noise\n",
    "\n",
    "        # truncate negative values of Y    \n",
    "        np.clip(Y, a_min=0, a_max=None, out=Y)\n",
    "\n",
    "        return X, Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_tobit_dataset(n_points, n_features, train_test_split=4):\n",
    "    \"\"\"\n",
    "    Generate dataset for Tobit regression model and split it into training and test portions\n",
    "    \n",
    "    \"\"\"\n",
    "    # n_features + 1 weights (one for a constant feature)\n",
    "    data_weights = np.array([-0.25, 0.5, 0.2, .1]) \n",
    "    noise_std = 0.1\n",
    "    \n",
    "    # Generate dataset\n",
    "    X, Y = Tobit_Regression.generate_data(n_points=n_points,\n",
    "                                           n_features=n_features,\n",
    "                                           weights=data_weights,\n",
    "                                           noise_std=noise_std)\n",
    "    \n",
    "    # split to the train and test set\n",
    "    # 1/4 of the data is used for a test\n",
    "    \n",
    "    n_test = int(n_points / train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def train_model(n_features, learning_rate, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Train Tobit Regression model\n",
    "    \n",
    "    Return:\n",
    "        a tuple of:\n",
    "        - Model fitted weights, np.array\n",
    "        - loss, double \n",
    "        - fitted noise std error, double\n",
    "        - L1 error, double\n",
    "    \"\"\"\n",
    "    # create an instance of the Tobit Regression class  \n",
    "    model = Tobit_Regression(n_features=n_features, learning_rate=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for _ in range(0, n_steps):\n",
    "            (_, loss), weights = sess.run((model.train_step, model.weights), feed_dict={\n",
    "                model.input: X_train,\n",
    "                model.target: Y_train\n",
    "                })\n",
    "    \n",
    "        # predictions for the test set\n",
    "        # std_model = weights[-1]**2     \n",
    "        output, std_model = sess.run([model.output,model.sigma], \n",
    "                                     feed_dict={model.input: X_test})\n",
    "        \n",
    "        output_L1_error = sess.run(model.output_L1_error,\n",
    "                                   feed_dict={model.input: X_test,\n",
    "                                   model.target: Y_test})\n",
    "        sess.close()\n",
    "    return weights[:-1], loss, std_model[0], output_L1_error, output\n",
    "\n",
    "def plot_results():        \n",
    "    # Plot a projection of test prediction on the first two predictors\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_test[:,1], X_test[:,2], Y_test, s=1, c=\"#000000\")\n",
    "    ax.scatter(X_test[:,1], X_test[:,2], output.reshape([-1,1]), s=1, c=\"#FF0000\")\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "n_points = 5000\n",
    "n_features = 3\n",
    "learning_rate = 0.05\n",
    "n_steps = 1000\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = gen_tobit_dataset(n_points, n_features)\n",
    "reset_graph()\n",
    "weights, loss, std_model, error_L1, output = train_model(n_features, learning_rate, n_steps)\n",
    "\n",
    "part_1=list(weights.squeeze())\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[0],all_parts,submissions)\n",
    "weights.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_2=[loss, std_model, error_L1]\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)  \n",
    "    \n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:2],all_parts,submissions)\n",
    "[loss, std_model, error_L1]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Linear regression and Neural Network to Non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(n_points=10000, n_features=3, use_nonlinear=True, \n",
    "                    noise_std=0.1, train_test_split = 4):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_points - number of data points to generate\n",
    "    n_features - a positive integer - number of features\n",
    "    use_nonlinear - if True, generate non-linear data\n",
    "    train_test_split - an integer - what portion of data to use for testing\n",
    "    \n",
    "    Return:\n",
    "    X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "    \"\"\"\n",
    "    # Linear data or non-linear data?\n",
    "    if use_nonlinear:\n",
    "        weights = np.array([[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]], dtype=np.float32)\n",
    "    else:\n",
    "        weights = np.array([1.0, 0.5, 0.2], dtype=np.float32)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    bias = np.ones(n_points).reshape((-1,1))\n",
    "    low = - np.ones((n_points,n_features), dtype=np.float32)\n",
    "    high = np.ones((n_points,n_features), dtype=np.float32)\n",
    "    \n",
    "    X = np.random.uniform(low=low, high=high)\n",
    "    noise = np.random.normal(size=(n_points, 1))\n",
    "    noise_std = 0.1\n",
    "    \n",
    "    if use_nonlinear:\n",
    "        Y = (weights[0,0] * bias + np.dot(X, weights[0, :]).reshape((-1,1)) + \n",
    "             np.dot(X*X, weights[1, :]).reshape([-1,1]) +\n",
    "             noise_std * noise)\n",
    "    else:\n",
    "        Y = (weights[0] * bias + np.dot(X, weights[:]).reshape((-1,1)) + \n",
    "             noise_std * noise)\n",
    "    \n",
    "    n_test = int(n_points/train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear=False)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear=True)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "Implement sklearn_lin_regress() function which returns a tuple of\n",
    "\n",
    "- coefficients of linear regression\n",
    "- an instance of LinearRegression class trained to X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sklearn_lin_regress\n",
    "def sklearn_lin_regress(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return: a tuple of \n",
    "      - np.array of size (k+1 by 1) of regression coefficients\n",
    "      - an instance of LinearRegression\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr_model = None\n",
    "    theta_sklearn = np.array([], dtype=np.float32)\n",
    "    ### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return theta_sklearn, lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_sklearn, lr_model = sklearn_lin_regress(X_train, Y_train)\n",
    "\n",
    "part_3 = list(theta_sklearn.squeeze())\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "    \n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:3],all_parts,submissions)\n",
    "\n",
    "theta_sklearn.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression.score() computes $R^2$ coefficient. The coefficient $R^2$ is defined as $(1 - \\frac{u}{v})$, where u is the residual sum of squares $\\sum (y\\_true - y\\_pred)^2$ and v is the total sum of squares $\\sum (y\\_true - \\bar{y\\_true})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "# calculate Linear Regression score\n",
    "model_score = 0.\n",
    "if lr_model is not None:\n",
    "    model_score = lr_model.score(X_test, Y_test)\n",
    "part4=str(model_score)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:4],all_parts,submissions)\n",
    "model_score\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with Tensorflow \n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Construct two-layer Neural Network utilizing neuron_layer() function. The number of nodes in two hidden layers are defined by n_hidden1 and n_hidden2, respectively. Use Gradient Descent Optimizer.\n",
    "\n",
    "The train the network using X_train / y_train and compute accuracy of the prediction using X_test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch\n",
    "    \n",
    "def neuron_layer(X, n_neurons, name, activation_fn=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation_fn is not None:\n",
    "            return activation_fn(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = 100\n",
    "n_hidden2 = 120\n",
    "n_outputs = 1 # single value prediction\n",
    "n_inputs = X_test.shape[1]\n",
    "\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "### START CODE HERE ### (≈ 10-15 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 200\n",
    "batch_size = 60\n",
    "num_rec = X_train.shape[0]\n",
    "n_batches = int(np.ceil(num_rec / batch_size))\n",
    "acc_test = 0. #  assign the result of accuracy testing to this variable\n",
    "\n",
    "### START CODE HERE ### (≈ 9-10 lines of code)\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part5=str(acc_test)\n",
    "submissions[all_parts[4]]=part5\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:5],all_parts,submissions)\n",
    "acc_test\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "guided-tour-machine-learning-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
